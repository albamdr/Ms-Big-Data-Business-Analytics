{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enunciado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejercicio se va a imaginar que se trabaja para una empresa de envíos de comida, presente en todo el territorio nacional, con miles de pedidos cada día. Dicha empresa tiene un fichero histórico con todas las peticiones de comida que los clientes han realizado mediante el chat de su web en los últimos meses. Necesitan __analizar en tiempo real qué comidas están pidiendo los usuarios y qué ingredientes tenían__, ya que en la cadena de stock de alimentos es necesario realizar una previsión para no quedarse sin platos cocinados. Se ha calculado que el impacto en las ventas cada vez que uno de los platos deja de estar disponible es del 7% de pérdidas en esa semana, debido al abandono de la web de pedidos por parte del cliente. Por tanto, es de vital importancia poder realizar automáticamente estimaciones al respecto.\n",
    "\n",
    "El objetivo es programar una __función que reciba como input un texto de usuario y devuelva los fragmentos de texto (chunks) que hagan referencia a las comidas y cantidades que ha solicitado__. No es necesario, ni es el objetivo de este ejercicio, construir un clasificador de intención previo a esta función, sino simplemente una función que presuponemos recibe una frase con la intención 'Pedir_comida'. Tampoco es objetivo normalizar la salida (por ej.: no es necesario convertir 'tres' a '3' ni 'pizzas' a 'pizza'). Es, por tanto, un ejercicio de mínimos.\n",
    "\n",
    "Por tanto, la __salida de la función__ será un __array con diccionarios de 2 elementos (comida y cantidad)__. Cuando una cantidad no sea detectada, se pondrá su valor a '1' como valor por defecto.\n",
    "\n",
    "El alumno deberá usar un __NaiveBayesClassifier__, en lugar del MaxEntClassifier, para localizar los elementos descritos anteriormente (comida y cantidad). Si el alumno no es capaz de construir un NaiveBayesClassifier —necesario para obtener un 10 en la práctica—, puede realizarlo mediante unigram o bigram tagger —para obtener un 9— o si no mediante RegexParser —un 7—.\n",
    "\n",
    "Se deberá comenzar la práctica por el nivel más básico de dificultad (RegexParser) y, en caso de conseguirlo, añadir los siguientes niveles de forma sucesiva. De esta forma, el entregable contendrá todas y cada una de las tres formas de solucionar el problema. No basta, por tanto, con incluir, por ejemplo, únicamente un NaiveBayesClassifier, hay que incluir también las otras dos formas si se quiere obtener la máxima puntuación. Se trata simplemente de una práctica y, por tanto, no se espera como resultado un sistema de alta precisión listo para usar en producción, sino simplemente una aproximación básica que permita ejecutar las tres formas de resolver el problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pprint\n",
    "from nltk.chunk import *\n",
    "from nltk.chunk.util import *\n",
    "from nltk.chunk.regexp import *\n",
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Se crea un corpus de frases de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['Me pones un pincho de tortilla, por favor?',\n",
    "          'quiero una pizza margherita',\n",
    "          'quiero un perrito caliente con patatas',\n",
    "          'voy a pedir un sandwich de atún',\n",
    "          'quiero 2 manzanas y una tarta de limón',\n",
    "          'Queremos tres tortillas de patata y unos macarrones con queso',\n",
    "          'una sopa de verduras',\n",
    "          'Un plato de pescado',\n",
    "          'quiero pedir tres raciones de calamares y dos de croquetas',\n",
    "          'pizza con peperoni y aceitunas, gracias',\n",
    "          'Tomaré una mousse de limón',\n",
    "          'Bocadillo de jamón y queso',\n",
    "          'quiero pedir un sandwich de atún y 3 hamburguesas con queso',\n",
    "          'Me gustaría pollo asado y tres bocadillos de tortilla, muchas gracias',\n",
    "          'quiero dos raciones de calamares',\n",
    "          'tallarines carbonara',\n",
    "          'tortilla de calabacín',\n",
    "          'Dos bizcochos de naranja',\n",
    "          'dos pizzas de peperoni y una pizza de queso, por favor',\n",
    "          'Quiero una pizza con anchoas',\n",
    "          'quiero pedir 4 bocadillo de jamón',\n",
    "          'Me pones un arroz con tomate',\n",
    "          'me gustaría una paella',\n",
    "          'Dos filetes empanados con patatas y un bocadillo de jamón, gracias',\n",
    "          'Quiero pedir una tarta de manzana',\n",
    "          'un batido de chocolate y tres tartas de  zanahoria',\n",
    "          'Una ración de queso y jamón',\n",
    "          'Tres galletas de coco',\n",
    "          'quiero un mousse de chocolate y un batido de coco',\n",
    "          'Un plato de chipirones',\n",
    "          'Quiero 2 flanes con nata',\n",
    "          'Dos naranjas y una manzana',\n",
    "          'Me pones un bizcocho con nueces y dos galletas de chocolate',\n",
    "          'me pones una tostada de aguacate y 1 tostada de salmón',\n",
    "          'Voy a pedir salmón con verduras y una pizza carbonara',\n",
    "          'Quiero dos platos de macarrones con tomate y un filete de ternera',\n",
    "          'Me gustaría una sopa de marisco',\n",
    "          'Quiero dos perritos calientes y unas patatas con queso y bacon',\n",
    "          'Me pones dos filetes de lomo con patatas',\n",
    "          'Quiero un puré de calabaza y un filete de pescado',\n",
    "          'Unos nuggets y unas patatas fritas',\n",
    "          'Quiero dos sopas de pescado y un solomillo',\n",
    "          'Me pones 4 platos de pasta con tomate y 2 filetes de pescado',\n",
    "          'quiero macarrones con queso',\n",
    "          'Vamos a pedir tres batidos de plátano',\n",
    "          'una hamburguesa con bacon y queso',\n",
    "          'Dos ensaladas con aceitunas y anchoas',\n",
    "          'tres pizzas con ajo y una lasaña',\n",
    "          'Quiero cuatro raciones de carne estofada',\n",
    "          'Me gustaría pedir dos solomillos con patatas',\n",
    "          'Nachos con guacamole y queso',\n",
    "          'Una tarta de chocolate y una tarta de queso',\n",
    "          'Queremos tres ensaladas con tomate y jamón y dos bocadillos de lomo',\n",
    "          'un helado de vainilla',\n",
    "          'Un yogur de limón y una tarta de chocolate',\n",
    "          'quiero guisantes con jamón',\n",
    "          'dos filetes de pescado y unos espaguetis con tomate',\n",
    "          'una sopa de fideos',\n",
    "          'pediré 3 batidos de vainilla y 1 helado de fresa',\n",
    "          'quiero pedir pasta con tomate y filetes rusos',\n",
    "          'voy a pedir 2 filetes de pescado, 1 hamburguesa con bacon y queso y 2 tortillas con calabacín',\n",
    "          'un huevo frito con patatas y dos bocadillos de jamón y queso, por favor',\n",
    "          'dos perritos calientes y un sandwich de atún y queso',\n",
    "          'arroz con tomate y filete de lomo con patatas, gracias',\n",
    "          'vamos a pedir tres galletas con chocolate, 2 tartas de limón y 1 café',\n",
    "          'quiero pedir 3 cafés con leche, 2 tartas de zanahoria, un flan y 2 helados de chocolate, por favor',\n",
    "          'ponme 2 filetes con patatas y una cerveza',\n",
    "          'tres tartas de queso',\n",
    "          'quiero dos manzanas y una pera',\n",
    "          'Queremos dos bocadillos de tortilla y dos bocadillos de jamón',\n",
    "          'un sandwich de lomo',\n",
    "          'ensalada de aguacate y filete de pollo',\n",
    "          'Quería un filete de lomo',\n",
    "          'Quiero dos hamburguesas con cebolla y bacon',\n",
    "          'dos zumos de naranja y dos tostadas con tomate',\n",
    "          'dos refrescos, tres cervezas y ensalada de aguacate',\n",
    "          'Tres cafés y una galleta con chocolate',\n",
    "          'queremos un bocadillo de jamón, un boadillo de queso, un bocadillo de lomo y tres bocadillos de tortilla',\n",
    "          'Dos helados de chocolate',\n",
    "          'Quiero una ensalada con anchoas',\n",
    "          'voy a pedir puré de calabaza',\n",
    "          'quiero dos tostadas con salchichón',\n",
    "          'quiero lubina con verduras',\n",
    "          'me pones dos ensaladas con pollo y 3 filetes de lomo',\n",
    "          'voy a pedir patatas fritas',\n",
    "          'quiero que me pongas un solomillo, un filete de lomo con patatas y 2 tartas de queso',\n",
    "          'traeme 3 bizcochos de naranja',\n",
    "          'ponme un puré de verduras, 2 arroz con tomate y filete con patatas',\n",
    "          'quiero ensaladilla rusa',\n",
    "          '2 raciones de jamón y queso',\n",
    "          'vamos a pedir 5 cervezas y 5 hamburguesas con queso y bacon',\n",
    "          'bizcocho de almendras',\n",
    "          'dos ensaladas con cacahuetes y una pizza con aceitunas',\n",
    "          'Tres ensaladas con queso y dos huevos fritos, gracias',\n",
    "          'Tomaré una mandarina y un plátano',\n",
    "          'Un batido de fresa y dos yogures de coco',\n",
    "          'Cinco purés de verduras y dos filetes de ternera con patatas',\n",
    "          'quiero pedir melón con jamón',\n",
    "          'risotto con queso',\n",
    "          'dos sandwiches de queso y cebolla, 1 bocadillo de chorizo y una hamburguesa de pollo',\n",
    "          'Quiero tres mandarinas, una manzana y dos tartas de queso',\n",
    "          'Patatas con queso y bacon',\n",
    "          'ponme 2 hamburguesas con bacon, unas patatas fritas y unos aros de cebolla',\n",
    "          'quiero pedir nachos con queso y una cerveza',\n",
    "          'me pones un batido de fresa, un zumo de melocotón y 2 tartas de manzana',\n",
    "          '3 raciones de queso y una ración de croquetas',\n",
    "          'un solomillo con pimientos y una lubina',\n",
    "          'quiero pedir pasta con tomate y una pizza carbonara',\n",
    "          'queremos pan con ajo, pasta boloñesa y pizza con anchoas',\n",
    "          'Pasta con aguacate y gambas',\n",
    "          'quiero un risotto con champiñones',\n",
    "          'quiero pedir 2 raciones de croquetas y un arroz con verduras',\n",
    "          'Voy a pedir tres platos de arroz con gambas',\n",
    "          'Quiero dos tostadas con tomate y aguacate',\n",
    "          'Me gustaría pedir unas gambas',\n",
    "          'quiero unos tallarines con gambas y un rollito de verduras, por favor',\n",
    "          'Me pone tres platos de filetes con guisantes',\n",
    "          'Querría dos filetes empanados con verduras',\n",
    "          'quiero tres filetes de pollo y 2 helados de fresa',\n",
    "          'vamos a pedir una cerveza, un refresco, un zumo de naranja y dos raciones de patatas fritas',\n",
    "          'fresas con nata',\n",
    "          'voy a pedir dos fajitas de ternera y unos nachos con queso',\n",
    "          'tres macarrones carbonara',\n",
    "          'quiero dos burritos y unos nachos',\n",
    "          'Vamos a pedir una hamburguesa con queso y dos pizzas con anchoas y peperoni',\n",
    "          'Nos gustaría tomar dos sopas de fideos y una ensalada césar',\n",
    "          'Pediré unos aros de cebolla y una hamburguesa con bacon',\n",
    "          'Quiero una sopa castellana',\n",
    "          'Vamos a pedir tres filetes con puré de patata',\n",
    "          'quiero dos bocadillos de salchicas y un bocadillo de tortilla, gracias',\n",
    "          'una tortilla de patatas y una ensalada',\n",
    "          'Dos perritos calientes y unas patatas con queso',\n",
    "          'Quiero cuatro tiramisús',\n",
    "          'voy a pedir dos refrescos, un bocadillo de calamares y un filete de lomo',\n",
    "          'me pones un pincho de tortilla, un puré de calabacín y un pollo asado',\n",
    "          'quiero que me traigas dos hamburguesas con cebolla y unas patatas fritas',\n",
    "          'voy a pedir ensalada de tomate, 2 hamburguesas de pollo y tarta de chocolate',\n",
    "          'quiero pedir guisantes con jamón y melón, por favor',\n",
    "          '2 batidos de chocolate, 1 zumo de naranja y 2 bocadillos de queso',\n",
    "          'quiero un risotto de setas y un solomillo con verduras']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entrenar un tagger para el español."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos todas las frases anotadas del corpus CESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cess_esp\n",
    "\n",
    "sents = cess_esp.tagged_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un conjunto de entrenamiento y otro de prueba.\n",
    "\n",
    "Metemos en el conjunto de entrenamiento el 90% de las frases, y el restante 10% en el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "test = []\n",
    "\n",
    "for i in range(len(sents)):\n",
    "    if i % 10:\n",
    "        training.append(sents[i])\n",
    "    else:\n",
    "        test.append(sents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos un POS tagger con el corpus en español, en este caso se ha elegido un __Hidden Markov Models__ porque es el más completo y no necesita de backoffs pues basa la asignación de etiquetas a cada palabra en la probabilidad conjunta que tiene la frase completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.hmm import HiddenMarkovModelTagger\n",
    "\n",
    "hmm_tagger = HiddenMarkovModelTagger.train(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos sobre el conjunto de test que no usamos para el entrenamiento, para ver qué porcentaje de acierto hemos conseguido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acierto con HMMs: 89.88905831011094\n"
     ]
    }
   ],
   "source": [
    "print ('Acierto con HMMs:',hmm_tagger.evaluate(test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probamos el tagger con las 5 primeras frases de nuestro corpus de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Análisis con HMM de la frase: \u001b[0m Me pones un pincho de tortilla, por favor? \n",
      "\n",
      "[('Me', 'pp1cs000'), ('pones', 'vmip3s0'), ('un', 'di0ms0'), ('pincho', 'ncms000'), ('de', 'sps00'), ('tortilla', 'ncfs000'), (',', 'Fc'), ('por', 'sps00'), ('favor', 'ncms000'), ('?', 'Fit')] \n",
      "\n",
      "\u001b[1m Análisis con HMM de la frase: \u001b[0m quiero una pizza margherita \n",
      "\n",
      "[('quiero', 'sps00'), ('una', 'di0fs0'), ('pizza', 'ncfs000'), ('margherita', 'aq0fs0')] \n",
      "\n",
      "\u001b[1m Análisis con HMM de la frase: \u001b[0m quiero un perrito caliente con patatas \n",
      "\n",
      "[('quiero', 'sps00'), ('un', 'di0ms0'), ('perrito', 'ncms000'), ('caliente', 'aq0cs0'), ('con', 'sps00'), ('patatas', 'ncfp000')] \n",
      "\n",
      "\u001b[1m Análisis con HMM de la frase: \u001b[0m voy a pedir un sandwich de atún \n",
      "\n",
      "[('voy', 'rg'), ('a', 'sps00'), ('pedir', 'vmn0000'), ('un', 'di0ms0'), ('sandwich', 'ncms000'), ('de', 'sps00'), ('atún', 'da0fs0')] \n",
      "\n",
      "\u001b[1m Análisis con HMM de la frase: \u001b[0m quiero 2 manzanas y una tarta de limón \n",
      "\n",
      "[('quiero', 'da0mp0'), ('2', 'Z'), ('manzanas', 'ncmp000'), ('y', 'cc'), ('una', 'di0fs0'), ('tarta', 'ncfs000'), ('de', 'sps00'), ('limón', 'da0fs0')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in corpus[:5]:\n",
    "    \n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged_HMM = hmm_tagger.tag(tokens)\n",
    "    \n",
    "    print('\\033[1m Análisis con HMM de la frase: \\033[0m', sentence, '\\n')\n",
    "    print(tagged_HMM, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Construir un Regex Parser que detecte comidas y cantidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos las expresiones regulares necesarias para reconocer cúando hablamos de comidas y cantidades. La estrucutura que queremos identificar es la siguiente:\n",
    "- Numeral cardinal (cuatro, cinco) o determinante indefinido o numeral (un, una) o un número entero (2, 3) antecediendo al alimento que nos va a indicar la __cantidad__, ej '__una__ tortilla... ' (en ocasiones no aparece, entonces supondremos que es 1).\n",
    "- Sustantivo común que define el __alimento__, ej '__tortilla__...'.\n",
    "- Sustantivo común, det. adj. o adj. calificativo que acompaña al alimento y se refiere al __ingrediente__, ej 'tortilla de __patata__...'.\n",
    "\n",
    "Las etiquetas que estamos buscando son:\n",
    "- Las etiquetas que pertenecen a __determinantes indefinidos__ son de la forma <code>'di___'</code>.\n",
    "- Las que pertenecen a __determinantes numerales__ son de la forma <code>'dn___'</code>\n",
    "- En el caso de __numerales cardinales__, tienen etiqueta de la forma <code>'mc___'</code>. \n",
    "- Para números enteros, las etiquetas son <code>'Z'</code>. \n",
    "- Las etiquetas que pertenecen a __sustantivos comunes__ son de la forma <code>'nc___'</code>. \n",
    "- Las etiquetas que pertenecen a __det. adj.__ son de la forma <code>'da___'</code>. \n",
    "- Las etiquetas que pertenecen a __Adj. calificativo__ son de la forma <code>'aq___'</code>.\n",
    "\n",
    "Entonces, para las cantidades nos quedamos con: \n",
    "- Det. indef (ej: un, una): <di.*>\n",
    "- Det. num (ej: una, dos): <dn.*>\n",
    "- Num. cardinal (ej: cuatro, cinco): <mc.*>\n",
    "- Números enteros (ej: 2, 3): <Z\\> \n",
    "\n",
    "Para las comidas nos quedamos con: \n",
    "- Nombre común (ej: tortilla): <nc.\\*>\n",
    "\n",
    "Para los ingredientes nos quedamos con: \n",
    "- Nombre común (ej: anchoas): <nc.\\*>\n",
    "- Det. adj (ej: limón): <da.\\*>\n",
    "- Adj. calificativo (ej: margherita): <aq.\\*>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    cantidad: {<di.*>|<dn.*>|<mc.*>|<Z>}\n",
    "    comida: {<nc.*>}\n",
    "    ingrediente: {<nc.*>|<da.*>|<aq.*>}\n",
    "\"\"\"\n",
    "\n",
    "regex_parser = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  voy/rg\n",
      "  a/sps00\n",
      "  pedir/vmn0000\n",
      "  (cantidad tres/dn0cp0)\n",
      "  (comida bocadillos/ncmp000)\n",
      "  de/sps00\n",
      "  anchoas/np0000l\n",
      "  y/cc\n",
      "  (cantidad 2/Z)\n",
      "  (comida manzanas/ncmp000))\n"
     ]
    }
   ],
   "source": [
    "#probamos el regex_parser con la gramática que hemos creado\n",
    "\n",
    "sentence = 'voy a pedir tres bocadillos de anchoas y 2 manzanas'\n",
    "sentence_tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sentence = hmm_tagger.tag(sentence_tokens)\n",
    "\n",
    "r = regex_parser.parse(tagged_sentence)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que no reconoce anchoas como ingrediente, pero no es problema de la gramática si no del taggeado del hmm por ponerle como etiqueta que es un nombre propio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la función que se pide en el enunciado, cuyo input es texto de usuario y devuleve un array con diccionarios de 3 elementos (cantidad, comida e ingredientes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_IE_comida():\n",
    "    sentence = input('¿Qué quieres pedir? ')\n",
    "    \n",
    "    arr = []\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    tagged_sentence = hmm_tagger.tag(sentence_tokens)\n",
    "    tree = regex_parser.parse(tagged_sentence)\n",
    "    \n",
    "    dic = {'cantidad':1, 'comida':'', 'ingrediente':''}\n",
    "\n",
    "    for subtree in tree.subtrees():\n",
    "        \n",
    "        #hemos terminado de registrar una comida cuando el campo de 'comida' en el diccionario\n",
    "        #es no vacío y no estamos en un subtree de ingrediente, entonces añadimos el diccionario\n",
    "        #al array e inicializamos uno nuevo\n",
    "        if (dic['comida'] != '') & (subtree.label() != 'ingrediente'):\n",
    "            arr.append(dic)\n",
    "            dic = {'cantidad':1, 'comida':'', 'ingrediente':''}\n",
    "            \n",
    "        if subtree.label() == 'cantidad':\n",
    "            dic['cantidad'] = subtree[0][0]\n",
    "\n",
    "        elif subtree.label() == 'comida':\n",
    "            dic['comida'] = subtree[0][0]\n",
    "\n",
    "        elif subtree.label() == 'ingrediente':\n",
    "            dic['ingrediente'] = dic['ingrediente']+', '+subtree[0][0]\n",
    "            \n",
    "    arr.append(dic)           \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Qué quieres pedir? quiero pedir 3 bocadillos de calamares y una ensalada con anchoas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'cantidad': '3', 'comida': 'bocadillos', 'ingrediente': ''},\n",
       " {'cantidad': 'una', 'comida': 'ensalada', 'ingrediente': ', anchoas'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probamos con una frase\n",
    "\n",
    "regex_IE_comida()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que no reconoce calamares como ingrediente, vamos a analizar la frase por separado para ver qué puede estar pasando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  quiero/np0000p\n",
      "  pedir/Fpa\n",
      "  (cantidad 3/Z)\n",
      "  (comida bocadillos/ncmp000)\n",
      "  de/sps00\n",
      "  calamares/np0000l\n",
      "  y/cc\n",
      "  (cantidad una/di0fs0)\n",
      "  (comida ensalada/ncfs000)\n",
      "  con/sps00\n",
      "  (ingrediente anchoas/da0fs0))\n"
     ]
    }
   ],
   "source": [
    "sentence = 'quiero pedir 3 bocadillos de calamares y una ensalada con anchoas'\n",
    "\n",
    "sentence_tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sentence = hmm_tagger.tag(sentence_tokens)\n",
    "tree = regex_parser.parse(tagged_sentence)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema está en el tag del hmm ya que ha etiquetado calamares como nombre propio y los ingredientes son nombres comunes, det. adj. o adj. calificativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Se usa el pos tagger y el Regex Parser para obtener las IOB de una nueva frase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para generar las IOB de una nueva frase vamos a utilizar la función <code>nltk.chunk.tree2conlltags</code> que genera automáticamente las IOB de un árbol taggeado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('quiero', 'sps00', 'O'), ('pedir', 'vmn0000', 'O'), ('dos', 'dn0cp0', 'B-cantidad'), ('batidos', 'ncmp000', 'B-comida'), ('de', 'sps00', 'O'), ('chocolate', 'ncms000', 'B-comida'), ('y', 'cc', 'O'), ('una', 'di0fs0', 'B-cantidad'), ('tarta', 'ncfs000', 'B-comida'), ('de', 'sps00', 'O'), ('limón', 'da0fs0', 'B-ingrediente')]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'quiero pedir dos batidos de chocolate y una tarta de limón'\n",
    "sentence_tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sentence = hmm_tagger.tag(sentence_tokens)\n",
    "tree = regex_parser.parse(tagged_sentence)\n",
    "print(tree2conlltags(tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el tagging es correcto ya que los chunks son de __una palabra__ y los taggea como __B-chunk__, si tuvieran más de una palabra la primera sería B-chunk y el resto I-chunk, pero no es el caso. El único error que podríamos detectar es que ha taggeado chocolate como comida y no como ingrediente, pero esto es por la ambigüedad que hay en la descripción de la gramática ya que tanto comidas como ingredientes pueden ser nombres comunes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con estos pasos, ya tendríamos cubierta la versión básica de la práctica.\n",
    "\n",
    "No obstante, por último, se sugiere al alumno que use el pos tagger y el Regex Parser para crear un corpus IOB que sirva para entrenar los bigram taggers o el NaiveBayesClassifier.\n",
    "\n",
    "Este corpus debe contener las __frases__ con las queremos entrenar y testear __taggeadas con el RegexpParser__ para que encuentre los chunks y en __forma de árbol__, ya que en el entrenamiento el tagger pasara de árbol a tripletas (word, POS, IOB) con la función <code>tree2conlltags</code> y en el evaluate generará una nueva etiqueta IOB con el tagger que ha creado, lo convertirá en un nuevo árbol y lo comparará con el árbol correspondiente del corpus de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedCorpus = []\n",
    "\n",
    "for sentence in corpus:\n",
    "\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    tagged_sentence = hmm_tagger.tag(sentence_tokens)\n",
    "    tree = regex_parser.parse(tagged_sentence)\n",
    "    #añadimos el árbol directamente ya que los taggers entrenan con árboles con los chunks etiquetados\n",
    "    parsedCorpus.append(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Me/pp1cs000\n",
      "  pones/vmip3s0\n",
      "  (cantidad un/di0ms0)\n",
      "  (comida pincho/ncms000)\n",
      "  de/sps00\n",
      "  (comida tortilla/ncfs000)\n",
      "  ,/Fc\n",
      "  por/sps00\n",
      "  (comida favor/ncms000)\n",
      "  ?/Fit)\n",
      "(S\n",
      "  quiero/sps00\n",
      "  (cantidad una/di0fs0)\n",
      "  (comida pizza/ncfs000)\n",
      "  (ingrediente margherita/aq0fs0))\n",
      "(S\n",
      "  quiero/sps00\n",
      "  (cantidad un/di0ms0)\n",
      "  (comida perrito/ncms000)\n",
      "  (ingrediente caliente/aq0cs0)\n",
      "  con/sps00\n",
      "  (comida patatas/ncfp000))\n"
     ]
    }
   ],
   "source": [
    "for tree in parsedCorpus[:3]:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos el corpus en 75% train 25% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = parsedCorpus[:105]\n",
    "corpus_test = parsedCorpus[105:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UnigramTagger\n",
    "\n",
    "Vamos a entrenar un Unigramtagger con el corpus etiquetado en forma de árbol.\n",
    "\n",
    "Utilizamos el código disponible en el capítulo 7 del libro de NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):   \n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_chunker = UnigramChunker(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy: 100.0%%\n",
      "    Precision:    100.0%%\n",
      "    Recall:       100.0%%\n",
      "    F-Measure:    100.0%%\n"
     ]
    }
   ],
   "source": [
    "print(unigram_chunker.evaluate(corpus_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos obtenido el 100% en todas las medidas, es decir, el UnigramTagger ha predicho correctamente todas las POS tags y IOB tags de los tokens del corpus de test (100% en IOB Accuracy). Así como todos los chunks están identificados correctamente también (100% en Precision, Recall y F-measure).\n",
    "\n",
    "Se espera, por tanto, que tanto el BigramTagger como el NaiveBayesClassifier obtengan estos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigramTagger\n",
    "\n",
    "Vamos a entrenar un Bigramtagger con el corpus etiquetado en forma de árbol.\n",
    "\n",
    "Adpatamos el código disponible en el capítulo 7 del libro de NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_chunker = BigramChunker(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  94.8%%\n",
      "    Precision:    100.0%%\n",
      "    Recall:        89.9%%\n",
      "    F-Measure:     94.7%%\n"
     ]
    }
   ],
   "source": [
    "print(bigram_chunker.evaluate(corpus_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el IOB accuracy es casi del 100% lo cual indica que ha debido de tener algún error al taggear alguna POS tag o IOB tag.\n",
    "\n",
    "Vemos que Precision es 100% pero Recall no llega al 100% por lo que no debe haber False Positives pero sí False Negatives.\n",
    "\n",
    "Esto podría corregirse añadiendo un backoff al BigramChunker. Sin embargo, se ha intentado añadir un backoff modificando el código que lo inicializa de la siguiente manera: \n",
    "\n",
    "<code>def __init__(self, train_sents, backoff):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data, backoff = backoff)</code>\n",
    "        \n",
    "y entrenándolo:\n",
    "\n",
    "<code>bigram_chunker = BigramChunker(corpus_train, backoff = unigram_chunker)</code>\n",
    "\n",
    "Sin embargo, esto produce un error de tipo:\n",
    "\n",
    "<code>AttributeError: 'UnigramChunker' object has no attribute '_taggers'</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función que se pide en el enunciado es análoga a la que se ha creado para el regexpParser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_IE_comida():\n",
    "    sentence = input('¿Qué quieres pedir? ')\n",
    "    \n",
    "    arr = []\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    tagged_sentence = hmm_tagger.tag(sentence_tokens)\n",
    "    tree = bigram_chunker.parse(tagged_sentence)\n",
    "\n",
    "    dic = {'cantidad':1, 'comida':'', 'ingrediente':''}\n",
    "\n",
    "    for subtree in tree.subtrees():\n",
    "        \n",
    "        #hemos terminado de registrar una comida cuando el campo de 'comida' en el diccionario\n",
    "        #es no vacío y no estamos en un subtree de ingrediente, entonces añadimos el diccionario\n",
    "        #al array e inicializamos uno nuevo\n",
    "        if (dic['comida'] != '') & (subtree.label() != 'ingrediente'):\n",
    "            arr.append(dic)\n",
    "            dic = {'cantidad':1, 'comida':'', 'ingrediente':''}\n",
    "            \n",
    "        if subtree.label() == 'cantidad':\n",
    "            dic['cantidad'] = subtree[0][0]\n",
    "\n",
    "        elif subtree.label() == 'comida':\n",
    "            dic['comida'] = subtree[0][0]\n",
    "\n",
    "        elif subtree.label() == 'ingrediente':\n",
    "            dic['ingrediente'] = dic['ingrediente']+', '+subtree[0][0]\n",
    "            \n",
    "    arr.append(dic)           \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Qué quieres pedir? quiero pedir 3 bocadillos de calamares y una ensalada con anchoas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'cantidad': '3', 'comida': 'bocadillos', 'ingrediente': ''},\n",
       " {'cantidad': 'una', 'comida': 'ensalada', 'ingrediente': ', anchoas'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_IE_comida()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que pasaba con el regexpParser no ha reconocido calamares como ingrediente ya que la etiqueta POS dice que es un nombre propio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaiveBayesClassifier \n",
    "\n",
    "Vamos a entrenar un NaiveBayesClassifier con el corpus etiquetado en forma de árbol.\n",
    "\n",
    "Adaptamos el código disponible en el capítulo 7 del libro de NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(\n",
    "            train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "                         nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo único que falta es el extractor de features (etiqueta POS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    return {\"pos\": pos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = ConsecutiveNPChunker(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy: 100.0%%\n",
      "    Precision:    100.0%%\n",
      "    Recall:       100.0%%\n",
      "    F-Measure:    100.0%%\n"
     ]
    }
   ],
   "source": [
    "print(chunker.evaluate(corpus_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se esperaba todas las medidas han resultado en el 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función que se pide en el enunciado es análoga a la que se ha creado para el regexpParser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_IE_comida():\n",
    "    sentence = input('¿Qué quieres pedir? ')\n",
    "    \n",
    "    arr = []\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    tagged_sentence = hmm_tagger.tag(sentence_tokens)\n",
    "    tree = chunker.parse(tagged_sentence)\n",
    "\n",
    "    dic = {'cantidad':1, 'comida':'', 'ingrediente':''}\n",
    "\n",
    "    for subtree in tree.subtrees():\n",
    "        \n",
    "        #hemos terminado de registrar una comida cuando el campo de 'comida' en el diccionario\n",
    "        #es no vacío y no estamos en un subtree de ingrediente, entonces añadimos el diccionario\n",
    "        #al array e inicializamos uno nuevo\n",
    "        if (dic['comida'] != '') & (subtree.label() != 'ingrediente'):\n",
    "            arr.append(dic)\n",
    "            dic = {'cantidad':1, 'comida':'', 'ingrediente':''}\n",
    "            \n",
    "        if subtree.label() == 'cantidad':\n",
    "            dic['cantidad'] = subtree[0][0]\n",
    "\n",
    "        elif subtree.label() == 'comida':\n",
    "            dic['comida'] = subtree[0][0]\n",
    "\n",
    "        elif subtree.label() == 'ingrediente':\n",
    "            dic['ingrediente'] = dic['ingrediente']+', '+subtree[0][0]\n",
    "            \n",
    "    arr.append(dic)           \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Qué quieres pedir? quiero pedir 3 bocadillos de calamares y una ensalada con anchoas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'cantidad': '3', 'comida': 'bocadillos', 'ingrediente': ''},\n",
       " {'cantidad': 'una', 'comida': 'ensalada', 'ingrediente': ', anchoas'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_IE_comida()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que pasaba con el regexpParser no ha reconocido calamares como ingrediente ya que la etiqueta POS dice que es un nombre propio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
